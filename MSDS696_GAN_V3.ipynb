{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamPeetz/PlaneGAN/blob/main/MSDS696_GAN_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lip5etIQEPQN",
        "outputId": "14a9052f-ce12-4434-e1b2-fc46a3f4e777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# example of a dcgan on cifar10\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from matplotlib import pyplot\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pathlib\n",
        "\n",
        "import os, shutil \n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrTxmPpaEnsx"
      },
      "outputs": [],
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(32,32,3)):\n",
        "\tmodel = Sequential()\n",
        "\t# normal\n",
        "\tmodel.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# downsample to 16x16\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# downsample to 8x8\n",
        "\tmodel.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# downsample to 4x4\n",
        "\tmodel.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# classifier\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDtG-SO4En3V"
      },
      "outputs": [],
      "source": [
        "# define the standalone generator model\n",
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 4x4 image\n",
        "\tn_nodes = 256 * 4 * 4\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((4, 4, 256)))\n",
        "\t# upsample to 8x8\n",
        "\tmodel.add(Conv2DTranspose(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 16x16\n",
        "\tmodel.add(Conv2DTranspose(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 32x32\n",
        "\tmodel.add(Conv2DTranspose(256, (3,3), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# output layer\n",
        "\tmodel.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDgVFRylEn-t"
      },
      "outputs": [],
      "source": [
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# connect them\n",
        "\tmodel = Sequential()\n",
        "\t# add generator\n",
        "\tmodel.add(g_model)\n",
        "\t# add the discriminator\n",
        "\tmodel.add(d_model)\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2tPw_SdEx7C"
      },
      "outputs": [],
      "source": [
        "# load and prepare cifar10 training images\n",
        "def load_real_samples():\n",
        "\t# load cifar10 dataset\n",
        "\t(trainX, _), (_, _) = load_data()\n",
        "\t# convert from unsigned ints to floats\n",
        "\tX = trainX.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\treturn X\n",
        "\n",
        "#load and prepare custom image dataset\n",
        "def create_real_img_dataset(image_path, resize_dim):\n",
        "\t\tdataset = []\n",
        "\t\tfor file in tqdm(os.listdir(image_path)):\n",
        "\t\t\t\timage = cv2.imread(os.path.join(image_path,file))\n",
        "\t\t\t\tX = image.astype('float32')\n",
        "\t\t\t\tX = X[:, :, [2, 1, 0]]\n",
        "\t\t\t\tX = (X - 127.5) / 127.5\n",
        "\t\t\t\tX = tf.image.resize(X, [resize_dim, resize_dim])\n",
        "\t\t\t\tdataset.append(X)\n",
        "\t\tdataset = np.array(dataset)\n",
        "\t\treturn dataset\n",
        "\n",
        "#load a custom image dataset with 1 level of directory\n",
        "# tensorflow documentation\n",
        "def load_custom_image_set(dataroot,resizedim,directorylevels):\n",
        "    #image parse function\n",
        "    def parse_image(filename):\n",
        "        parts = tf.strings.split(filename, os.sep)\n",
        "        label = parts[-2]\n",
        "        image = tf.io.read_file(filename)\n",
        "        image = tf.io.decode_jpeg(image)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        " #       image = tf.image.resize(image, [resizedim,resizedim])\n",
        " #       image = (image - 127.5) / 127.5\n",
        "        return image, label\n",
        "    \n",
        "    data_root = pathlib.Path(dataroot)\n",
        "    if directorylevels == 1:\n",
        "      list_ds = tf.data.Dataset.list_files(str(data_root/'*/*'))\n",
        "    if directorylevels == 2:\n",
        "      list_ds = tf.data.Dataset.list_files(str(data_root/'*/*/*'))\n",
        "    images_ds = list_ds.map(parse_image)\n",
        "    #https://stackoverflow.com/questions/70535683/extract-data-from-tensorflow-dataset-e-g-to-numpy\n",
        "    images = np.asarray(list(images_ds.map(lambda x, y: x)))\n",
        "    return images\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# modified with suggestion from Das Shuvo (2020)\n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\t# generate points in latent space\n",
        "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
        "\t# predict outputs\n",
        "\tinput_tensor = tf.convert_to_tensor(x_input)\n",
        "\tX = g_model(input_tensor)\n",
        "\tX = X.numpy()\n",
        "\t# create 'fake' class labels (0)\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2MWrLh8EyBL"
      },
      "outputs": [],
      "source": [
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, n=7):\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\texamples = (examples + 1) / 2.0\n",
        "#\texample = (examples + 1) * 255\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i])\n",
        "\t# save plot to file\n",
        "\tfilename = '/content/gdrive/My Drive/planegan/sample_output/generated_plot_e%03d.png' % (epoch+1)\n",
        "\tpyplot.savefig(filename)\n",
        "\tpyplot.close()\n",
        " \n",
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(2, 1, 1)\n",
        "\tpyplot.plot(d1_hist, label='d-real')\n",
        "\tpyplot.plot(d2_hist, label='d-fake')\n",
        "\tpyplot.plot(g_hist, label='gen')\n",
        "\tpyplot.legend()\n",
        "\t# plot discriminator accuracy\n",
        "\tpyplot.subplot(2, 1, 2)\n",
        "\tpyplot.plot(a1_hist, label='acc-real')\n",
        "\tpyplot.plot(a2_hist, label='acc-fake')\n",
        "\tpyplot.legend()\n",
        "\t# save plot to file\n",
        "\tpyplot.savefig('/content/gdrive/My Drive/planegan/saved_models/plot_line_plot_loss.png')\n",
        "\tpyplot.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrnRfx2tFKoi"
      },
      "outputs": [],
      "source": [
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "\t# prepare real samples\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\t# prepare fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\t# summarize discriminator performance\n",
        "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\t# save plot\n",
        "\tsave_plot(x_fake, epoch)\n",
        "\t# save the generator model tile file\n",
        "\tfilename = '/content/gdrive/My Drive/planegan/saved_models/generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(filename)\n",
        "\n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=300, n_batch=128):\n",
        "\tbat_per_epo = int(dataset.shape[0] / n_batch)\n",
        "\t# define half batches for training\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "  # prepare lists for storing stats each iteration\n",
        "\td1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\tX_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)\n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\tX_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)\n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "\t\t\tX_gan = generate_latent_points(latent_dim, n_batch)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
        "\t \t  # record training metrics\n",
        "\t\t\td1_hist.append(d_loss1)\n",
        "\t\t\td2_hist.append(d_loss2)\n",
        "\t\t\tg_hist.append(g_loss)\n",
        "\t\t\ta1_hist.append(d_acc1)\n",
        "\t\t\ta2_hist.append(d_acc2)\n",
        "\t\t# summarize loss\n",
        "\t\t\tif (j+1) % 5 == 0:\n",
        "\t\t\t\tprint('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "\t\t\t\t# record training metrics\n",
        "\t\t\t\td1_hist.append(d_loss1)\n",
        "\t\t\t\td2_hist.append(d_loss2)\n",
        "\t\t\t\tg_hist.append(g_loss)\n",
        "\t\t\t\ta1_hist.append(d_acc1)\n",
        "\t\t\t\ta2_hist.append(d_acc2)\n",
        "\t\t# evaluate the model performance, sometimes\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\t# save plot to directory\n",
        "\tplot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuVfdG7CFKtv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b98c0cf-f278-4fea-8fde-ba3ca5472644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "image_path = \"/content/gdrive/My Drive/CIFAR10/\"\n",
        "resizedim = 32\n",
        "directorylevels = 1\n",
        "dataset = load_custom_image_set(image_path,resizedim,directorylevels)\n",
        "# train model\n",
        "# train(g_model, d_model, gan_model, dataset, latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1 = (dataset -0.5)"
      ],
      "metadata": {
        "id": "JdqiPmiV5GCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_1[1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYCI8VUKySa4",
        "outputId": "b57398a1-79c2-4c08-f289-e0f8daadbdcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.25294116,  0.07254905, -0.09215683],\n",
              "       [-0.24117646,  0.05686277, -0.0960784 ],\n",
              "       [-0.37843138, -0.12352937, -0.25686273],\n",
              "       [-0.18235293,  0.06470591, -0.05686271],\n",
              "       [-0.30392155, -0.0333333 , -0.15490195],\n",
              "       [-0.15490195,  0.1156863 ,  0.00196081],\n",
              "       [-0.22941175,  0.02156866, -0.08431369],\n",
              "       [-0.3117647 , -0.06470585, -0.17058823],\n",
              "       [-0.272549  , -0.0215686 , -0.12745097],\n",
              "       [-0.29999998, -0.04117644, -0.15098038],\n",
              "       [-0.26862743,  0.00980395, -0.1078431 ],\n",
              "       [-0.11568624,  0.15882355,  0.04901963],\n",
              "       [-0.07254899,  0.19803923,  0.08823532],\n",
              "       [-0.26862743, -0.03725487, -0.12352937],\n",
              "       [-0.30784312, -0.13529411, -0.20196077],\n",
              "       [-0.29999998, -0.15882352, -0.21372548],\n",
              "       [-0.32352942, -0.19019607, -0.24509802],\n",
              "       [-0.4254902 , -0.2960784 , -0.3392157 ],\n",
              "       [-0.43333334, -0.27647057, -0.30784312],\n",
              "       [-0.42941177, -0.27647057, -0.28823528],\n",
              "       [-0.41764706, -0.2843137 , -0.27647057],\n",
              "       [-0.43333334, -0.29999998, -0.2960784 ],\n",
              "       [-0.48431373, -0.34313726, -0.3509804 ],\n",
              "       [-0.44117647, -0.2843137 , -0.3156863 ],\n",
              "       [-0.37058824, -0.1980392 , -0.2490196 ],\n",
              "       [-0.35882354, -0.1862745 , -0.24509802],\n",
              "       [-0.26862743, -0.1078431 , -0.17058823],\n",
              "       [-0.26862743, -0.09999996, -0.16666666],\n",
              "       [-0.31960785, -0.12745097, -0.20980391],\n",
              "       [-0.272549  , -0.03725487, -0.13921568],\n",
              "       [-0.3156863 , -0.02941173, -0.14705881],\n",
              "       [-0.32352942, -0.00196075, -0.14313725]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(g_model, d_model, gan_model, dataset_1, latent_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvS4kkVJyzxA",
        "outputId": "114ef90c-a222-40c8-cd18-7923f233311d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1, 5/390, d1=0.027, d2=0.949 g=0.781\n",
            ">1, 10/390, d1=0.752, d2=0.741 g=0.706\n",
            ">1, 15/390, d1=0.465, d2=0.889 g=0.639\n",
            ">1, 20/390, d1=0.791, d2=0.602 g=1.003\n",
            ">1, 25/390, d1=0.720, d2=0.718 g=0.687\n",
            ">1, 30/390, d1=0.586, d2=0.762 g=0.653\n",
            ">1, 35/390, d1=0.680, d2=0.660 g=0.877\n",
            ">1, 40/390, d1=0.719, d2=0.740 g=0.699\n",
            ">1, 45/390, d1=0.715, d2=0.667 g=0.778\n",
            ">1, 50/390, d1=0.694, d2=0.711 g=0.698\n",
            ">1, 55/390, d1=0.685, d2=0.727 g=0.705\n",
            ">1, 60/390, d1=0.710, d2=0.658 g=0.782\n",
            ">1, 65/390, d1=0.721, d2=0.689 g=0.720\n",
            ">1, 70/390, d1=0.723, d2=0.661 g=0.759\n",
            ">1, 75/390, d1=0.728, d2=0.679 g=0.726\n",
            ">1, 80/390, d1=0.705, d2=0.696 g=0.703\n",
            ">1, 85/390, d1=0.687, d2=0.705 g=0.695\n",
            ">1, 90/390, d1=0.668, d2=0.703 g=0.709\n",
            ">1, 95/390, d1=0.678, d2=0.698 g=0.736\n",
            ">1, 100/390, d1=0.704, d2=0.691 g=0.718\n",
            ">1, 105/390, d1=0.632, d2=0.691 g=0.733\n",
            ">1, 110/390, d1=0.718, d2=0.643 g=0.810\n",
            ">1, 115/390, d1=0.763, d2=0.623 g=0.799\n",
            ">1, 120/390, d1=0.726, d2=0.661 g=0.754\n",
            ">1, 125/390, d1=0.717, d2=0.703 g=0.712\n",
            ">1, 130/390, d1=0.713, d2=0.693 g=0.701\n",
            ">1, 135/390, d1=0.699, d2=0.677 g=0.744\n",
            ">1, 140/390, d1=0.664, d2=0.685 g=0.731\n",
            ">1, 145/390, d1=0.623, d2=0.717 g=0.740\n",
            ">1, 150/390, d1=0.691, d2=0.642 g=0.797\n",
            ">1, 155/390, d1=0.648, d2=0.689 g=0.719\n",
            ">1, 160/390, d1=0.637, d2=0.839 g=0.714\n",
            ">1, 165/390, d1=0.742, d2=0.649 g=0.748\n",
            ">1, 170/390, d1=0.719, d2=0.671 g=0.750\n",
            ">1, 175/390, d1=0.711, d2=0.643 g=0.784\n",
            ">1, 180/390, d1=0.708, d2=0.684 g=0.721\n",
            ">1, 185/390, d1=0.689, d2=0.687 g=0.774\n",
            ">1, 190/390, d1=0.689, d2=0.697 g=0.705\n",
            ">1, 195/390, d1=0.714, d2=0.687 g=0.760\n",
            ">1, 200/390, d1=0.719, d2=0.693 g=0.730\n",
            ">1, 205/390, d1=0.715, d2=0.675 g=0.721\n",
            ">1, 210/390, d1=0.685, d2=0.681 g=0.716\n",
            ">1, 215/390, d1=0.682, d2=0.693 g=0.716\n",
            ">1, 220/390, d1=0.657, d2=0.728 g=0.740\n",
            ">1, 225/390, d1=0.727, d2=0.655 g=0.755\n",
            ">1, 230/390, d1=0.730, d2=0.662 g=0.749\n",
            ">1, 235/390, d1=0.717, d2=0.657 g=0.735\n",
            ">1, 240/390, d1=0.648, d2=0.692 g=0.716\n",
            ">1, 245/390, d1=0.687, d2=0.691 g=0.733\n",
            ">1, 250/390, d1=0.714, d2=0.702 g=0.740\n",
            ">1, 255/390, d1=0.651, d2=0.789 g=0.730\n",
            ">1, 260/390, d1=0.702, d2=0.659 g=0.738\n",
            ">1, 265/390, d1=0.724, d2=0.685 g=0.729\n",
            ">1, 270/390, d1=0.659, d2=0.665 g=0.739\n",
            ">1, 275/390, d1=0.680, d2=0.695 g=0.747\n",
            ">1, 280/390, d1=0.703, d2=0.683 g=0.728\n",
            ">1, 285/390, d1=0.698, d2=0.669 g=0.750\n",
            ">1, 290/390, d1=0.582, d2=0.759 g=0.745\n",
            ">1, 295/390, d1=0.736, d2=0.673 g=0.757\n",
            ">1, 300/390, d1=0.714, d2=0.679 g=0.715\n",
            ">1, 305/390, d1=0.671, d2=0.704 g=0.717\n",
            ">1, 310/390, d1=0.662, d2=0.714 g=0.713\n",
            ">1, 315/390, d1=0.712, d2=0.672 g=0.742\n",
            ">1, 320/390, d1=0.691, d2=0.656 g=0.763\n",
            ">1, 325/390, d1=0.685, d2=0.736 g=0.758\n",
            ">1, 330/390, d1=0.727, d2=0.647 g=0.758\n",
            ">1, 335/390, d1=0.697, d2=0.687 g=0.711\n",
            ">1, 340/390, d1=0.670, d2=0.707 g=0.712\n",
            ">1, 345/390, d1=0.699, d2=0.636 g=0.769\n",
            ">1, 350/390, d1=0.689, d2=0.690 g=0.762\n",
            ">1, 355/390, d1=0.699, d2=0.670 g=0.753\n",
            ">1, 360/390, d1=0.686, d2=0.668 g=0.749\n",
            ">1, 365/390, d1=0.681, d2=0.690 g=0.747\n",
            ">1, 370/390, d1=0.728, d2=0.646 g=0.775\n",
            ">1, 375/390, d1=0.727, d2=0.647 g=0.766\n",
            ">1, 380/390, d1=0.630, d2=0.810 g=0.703\n",
            ">1, 385/390, d1=0.680, d2=0.706 g=0.749\n",
            ">1, 390/390, d1=0.711, d2=0.642 g=0.757\n",
            ">2, 5/390, d1=0.691, d2=0.695 g=0.708\n",
            ">2, 10/390, d1=0.716, d2=0.636 g=0.790\n",
            ">2, 15/390, d1=0.680, d2=0.691 g=0.717\n",
            ">2, 20/390, d1=0.698, d2=0.689 g=0.716\n",
            ">2, 25/390, d1=0.682, d2=0.684 g=0.719\n",
            ">2, 30/390, d1=0.700, d2=0.693 g=0.723\n",
            ">2, 35/390, d1=0.695, d2=0.693 g=0.711\n",
            ">2, 40/390, d1=0.717, d2=0.703 g=0.703\n",
            ">2, 45/390, d1=0.717, d2=0.678 g=0.733\n",
            ">2, 50/390, d1=0.715, d2=0.667 g=0.725\n",
            ">2, 55/390, d1=0.716, d2=0.654 g=0.814\n",
            ">2, 60/390, d1=0.699, d2=0.686 g=0.707\n",
            ">2, 65/390, d1=0.685, d2=0.696 g=0.697\n",
            ">2, 70/390, d1=0.667, d2=0.764 g=0.689\n",
            ">2, 75/390, d1=0.724, d2=0.667 g=0.735\n",
            ">2, 80/390, d1=0.702, d2=0.664 g=0.733\n",
            ">2, 85/390, d1=0.710, d2=0.705 g=0.713\n",
            ">2, 90/390, d1=0.709, d2=0.696 g=0.711\n",
            ">2, 95/390, d1=0.723, d2=0.684 g=0.708\n",
            ">2, 100/390, d1=0.706, d2=0.677 g=0.727\n",
            ">2, 105/390, d1=0.714, d2=0.661 g=0.744\n",
            ">2, 110/390, d1=0.686, d2=0.671 g=0.744\n",
            ">2, 115/390, d1=0.621, d2=0.731 g=0.673\n",
            ">2, 120/390, d1=0.708, d2=0.668 g=0.737\n",
            ">2, 125/390, d1=0.684, d2=0.702 g=0.713\n",
            ">2, 130/390, d1=0.645, d2=0.686 g=0.700\n",
            ">2, 135/390, d1=0.714, d2=0.646 g=0.754\n",
            ">2, 140/390, d1=0.690, d2=0.694 g=0.718\n",
            ">2, 145/390, d1=0.699, d2=0.681 g=0.720\n",
            ">2, 150/390, d1=0.710, d2=0.697 g=0.704\n",
            ">2, 155/390, d1=0.708, d2=0.689 g=0.707\n",
            ">2, 160/390, d1=0.697, d2=0.687 g=0.709\n",
            ">2, 165/390, d1=0.708, d2=0.685 g=0.714\n",
            ">2, 170/390, d1=0.701, d2=0.676 g=0.717\n",
            ">2, 175/390, d1=0.701, d2=0.689 g=0.713\n",
            ">2, 180/390, d1=0.705, d2=0.670 g=0.716\n",
            ">2, 185/390, d1=0.705, d2=0.693 g=0.714\n",
            ">2, 190/390, d1=0.710, d2=0.670 g=0.714\n",
            ">2, 195/390, d1=0.696, d2=0.691 g=0.711\n",
            ">2, 200/390, d1=0.703, d2=0.680 g=0.722\n",
            ">2, 205/390, d1=0.715, d2=0.677 g=0.708\n",
            ">2, 210/390, d1=0.697, d2=0.700 g=0.716\n",
            ">2, 215/390, d1=0.692, d2=0.666 g=0.728\n",
            ">2, 220/390, d1=0.707, d2=0.686 g=0.711\n",
            ">2, 225/390, d1=0.707, d2=0.706 g=0.698\n",
            ">2, 230/390, d1=0.690, d2=0.734 g=0.694\n",
            ">2, 235/390, d1=0.715, d2=0.678 g=0.726\n",
            ">2, 240/390, d1=0.698, d2=0.684 g=0.709\n",
            ">2, 245/390, d1=0.699, d2=0.686 g=0.710\n",
            ">2, 250/390, d1=0.698, d2=0.691 g=0.711\n",
            ">2, 255/390, d1=0.649, d2=0.785 g=0.693\n",
            ">2, 260/390, d1=0.708, d2=0.677 g=0.718\n",
            ">2, 265/390, d1=0.657, d2=0.735 g=0.676\n",
            ">2, 270/390, d1=0.701, d2=0.661 g=0.732\n",
            ">2, 275/390, d1=0.711, d2=0.682 g=0.724\n",
            ">2, 280/390, d1=0.709, d2=0.679 g=0.717\n",
            ">2, 285/390, d1=0.705, d2=0.678 g=0.714\n",
            ">2, 290/390, d1=0.701, d2=0.679 g=0.710\n",
            ">2, 295/390, d1=0.703, d2=0.690 g=0.713\n",
            ">2, 300/390, d1=0.699, d2=0.675 g=0.720\n",
            ">2, 305/390, d1=0.702, d2=0.689 g=0.704\n",
            ">2, 310/390, d1=0.696, d2=0.687 g=0.715\n",
            ">2, 315/390, d1=0.680, d2=0.675 g=0.711\n",
            ">2, 320/390, d1=0.660, d2=0.711 g=0.687\n",
            ">2, 325/390, d1=0.677, d2=0.698 g=0.700\n",
            ">2, 330/390, d1=0.694, d2=0.674 g=0.726\n",
            ">2, 335/390, d1=0.716, d2=0.682 g=0.708\n",
            ">2, 340/390, d1=0.713, d2=0.682 g=0.729\n",
            ">2, 345/390, d1=0.714, d2=0.679 g=0.717\n",
            ">2, 350/390, d1=0.699, d2=0.689 g=0.704\n",
            ">2, 355/390, d1=0.683, d2=0.699 g=0.703\n",
            ">2, 360/390, d1=0.685, d2=0.712 g=0.703\n",
            ">2, 365/390, d1=0.688, d2=0.674 g=0.710\n",
            ">2, 370/390, d1=0.691, d2=0.671 g=0.720\n",
            ">2, 375/390, d1=0.660, d2=0.705 g=0.711\n",
            ">2, 380/390, d1=0.696, d2=0.696 g=0.708\n",
            ">2, 385/390, d1=0.691, d2=0.708 g=0.709\n",
            ">2, 390/390, d1=0.690, d2=0.686 g=0.713\n",
            ">3, 5/390, d1=0.707, d2=0.659 g=0.773\n",
            ">3, 10/390, d1=0.708, d2=0.678 g=0.709\n",
            ">3, 15/390, d1=0.699, d2=0.691 g=0.716\n",
            ">3, 20/390, d1=0.715, d2=0.685 g=0.719\n",
            ">3, 25/390, d1=0.706, d2=0.722 g=0.713\n",
            ">3, 30/390, d1=0.699, d2=0.680 g=0.716\n",
            ">3, 35/390, d1=0.692, d2=0.703 g=0.696\n",
            ">3, 40/390, d1=0.708, d2=0.671 g=0.735\n",
            ">3, 45/390, d1=0.704, d2=0.681 g=0.718\n",
            ">3, 50/390, d1=0.677, d2=0.784 g=0.691\n",
            ">3, 55/390, d1=0.696, d2=0.666 g=0.724\n",
            ">3, 60/390, d1=0.677, d2=0.692 g=0.692\n",
            ">3, 65/390, d1=0.654, d2=0.722 g=0.680\n",
            ">3, 70/390, d1=0.704, d2=0.720 g=0.719\n",
            ">3, 75/390, d1=0.709, d2=0.668 g=0.726\n",
            ">3, 80/390, d1=0.690, d2=0.699 g=0.700\n",
            ">3, 85/390, d1=0.730, d2=0.671 g=0.725\n",
            ">3, 90/390, d1=0.701, d2=0.683 g=0.710\n",
            ">3, 95/390, d1=0.670, d2=0.708 g=0.689\n",
            ">3, 100/390, d1=0.649, d2=0.747 g=0.717\n",
            ">3, 105/390, d1=0.662, d2=0.683 g=0.722\n",
            ">3, 110/390, d1=0.699, d2=0.723 g=0.719\n",
            ">3, 115/390, d1=0.723, d2=0.675 g=0.755\n",
            ">3, 120/390, d1=0.702, d2=0.703 g=0.708\n",
            ">3, 125/390, d1=0.694, d2=0.697 g=0.703\n",
            ">3, 130/390, d1=0.687, d2=0.692 g=0.701\n",
            ">3, 135/390, d1=0.663, d2=0.713 g=0.678\n",
            ">3, 140/390, d1=0.672, d2=0.694 g=0.713\n",
            ">3, 145/390, d1=0.701, d2=0.676 g=0.725\n",
            ">3, 150/390, d1=0.711, d2=0.681 g=0.709\n",
            ">3, 155/390, d1=0.698, d2=0.700 g=0.703\n",
            ">3, 160/390, d1=0.693, d2=0.681 g=0.728\n",
            ">3, 165/390, d1=0.672, d2=0.690 g=0.710\n",
            ">3, 170/390, d1=0.690, d2=0.724 g=0.699\n",
            ">3, 175/390, d1=0.682, d2=0.694 g=0.706\n",
            ">3, 180/390, d1=0.692, d2=0.704 g=0.699\n",
            ">3, 185/390, d1=0.714, d2=0.670 g=0.734\n",
            ">3, 190/390, d1=0.718, d2=0.681 g=0.727\n",
            ">3, 195/390, d1=0.720, d2=0.674 g=0.724\n",
            ">3, 200/390, d1=0.705, d2=0.681 g=0.717\n",
            ">3, 205/390, d1=0.711, d2=0.696 g=0.728\n",
            ">3, 210/390, d1=0.705, d2=0.675 g=0.730\n",
            ">3, 215/390, d1=0.682, d2=0.701 g=0.712\n",
            ">3, 220/390, d1=0.691, d2=0.699 g=0.708\n",
            ">3, 225/390, d1=0.707, d2=0.695 g=0.714\n",
            ">3, 230/390, d1=0.704, d2=0.666 g=0.738\n",
            ">3, 235/390, d1=0.713, d2=0.676 g=0.730\n",
            ">3, 240/390, d1=0.703, d2=0.679 g=0.701\n",
            ">3, 245/390, d1=0.695, d2=0.695 g=0.703\n",
            ">3, 250/390, d1=0.699, d2=0.693 g=0.708\n",
            ">3, 255/390, d1=0.709, d2=0.686 g=0.707\n",
            ">3, 260/390, d1=0.703, d2=0.688 g=0.714\n",
            ">3, 265/390, d1=0.699, d2=0.681 g=0.710\n",
            ">3, 270/390, d1=0.701, d2=0.682 g=0.719\n",
            ">3, 275/390, d1=0.683, d2=0.699 g=0.709\n",
            ">3, 280/390, d1=0.699, d2=0.705 g=0.700\n",
            ">3, 285/390, d1=0.707, d2=0.677 g=0.721\n",
            ">3, 290/390, d1=0.690, d2=0.685 g=0.710\n",
            ">3, 295/390, d1=0.682, d2=0.694 g=0.703\n",
            ">3, 300/390, d1=0.697, d2=0.691 g=0.709\n",
            ">3, 305/390, d1=0.707, d2=0.678 g=0.721\n",
            ">3, 310/390, d1=0.706, d2=0.668 g=0.722\n",
            ">3, 315/390, d1=0.682, d2=0.694 g=0.689\n",
            ">3, 320/390, d1=0.689, d2=0.702 g=0.698\n",
            ">3, 325/390, d1=0.694, d2=0.690 g=0.715\n",
            ">3, 330/390, d1=0.709, d2=0.664 g=0.728\n",
            ">3, 335/390, d1=0.701, d2=0.692 g=0.699\n",
            ">3, 340/390, d1=0.680, d2=0.695 g=0.705\n",
            ">3, 345/390, d1=0.688, d2=0.697 g=0.697\n",
            ">3, 350/390, d1=0.686, d2=0.703 g=0.697\n",
            ">3, 355/390, d1=0.696, d2=0.691 g=0.701\n",
            ">3, 360/390, d1=0.707, d2=0.682 g=0.717\n",
            ">3, 365/390, d1=0.704, d2=0.675 g=0.721\n",
            ">3, 370/390, d1=0.695, d2=0.686 g=0.710\n",
            ">3, 375/390, d1=0.707, d2=0.675 g=0.721\n",
            ">3, 380/390, d1=0.709, d2=0.685 g=0.712\n",
            ">3, 385/390, d1=0.703, d2=0.692 g=0.714\n",
            ">3, 390/390, d1=0.697, d2=0.688 g=0.707\n",
            ">4, 5/390, d1=0.711, d2=0.681 g=0.713\n",
            ">4, 10/390, d1=0.696, d2=0.687 g=0.709\n",
            ">4, 15/390, d1=0.695, d2=0.681 g=0.710\n",
            ">4, 20/390, d1=0.709, d2=0.683 g=0.719\n",
            ">4, 25/390, d1=0.709, d2=0.690 g=0.716\n",
            ">4, 30/390, d1=0.695, d2=0.686 g=0.705\n",
            ">4, 35/390, d1=0.704, d2=0.693 g=0.714\n",
            ">4, 40/390, d1=0.710, d2=0.685 g=0.717\n",
            ">4, 45/390, d1=0.700, d2=0.681 g=0.714\n",
            ">4, 50/390, d1=0.697, d2=0.680 g=0.715\n",
            ">4, 55/390, d1=0.690, d2=0.680 g=0.718\n",
            ">4, 60/390, d1=0.699, d2=0.682 g=0.730\n",
            ">4, 65/390, d1=0.718, d2=0.670 g=0.725\n",
            ">4, 70/390, d1=0.702, d2=0.681 g=0.712\n",
            ">4, 75/390, d1=0.689, d2=0.691 g=0.699\n",
            ">4, 80/390, d1=0.694, d2=0.692 g=0.705\n",
            ">4, 85/390, d1=0.699, d2=0.686 g=0.700\n",
            ">4, 90/390, d1=0.702, d2=0.705 g=0.706\n",
            ">4, 95/390, d1=0.683, d2=0.691 g=0.704\n",
            ">4, 100/390, d1=0.695, d2=0.683 g=0.705\n",
            ">4, 105/390, d1=0.658, d2=0.728 g=0.695\n",
            ">4, 110/390, d1=0.697, d2=0.690 g=0.709\n",
            ">4, 115/390, d1=0.689, d2=0.694 g=0.705\n",
            ">4, 120/390, d1=0.694, d2=0.699 g=0.707\n",
            ">4, 125/390, d1=0.695, d2=0.695 g=0.702\n",
            ">4, 130/390, d1=0.697, d2=0.690 g=0.711\n",
            ">4, 135/390, d1=0.699, d2=0.680 g=0.719\n",
            ">4, 140/390, d1=0.675, d2=0.709 g=0.697\n",
            ">4, 145/390, d1=0.702, d2=0.681 g=0.718\n",
            ">4, 150/390, d1=0.708, d2=0.675 g=0.722\n",
            ">4, 155/390, d1=0.691, d2=0.684 g=0.708\n",
            ">4, 160/390, d1=0.687, d2=0.744 g=0.683\n",
            ">4, 165/390, d1=0.674, d2=0.668 g=0.755\n",
            ">4, 170/390, d1=0.701, d2=0.693 g=0.708\n",
            ">4, 175/390, d1=0.693, d2=0.687 g=0.705\n",
            ">4, 180/390, d1=0.699, d2=0.683 g=0.714\n",
            ">4, 185/390, d1=0.700, d2=0.694 g=0.700\n",
            ">4, 190/390, d1=0.685, d2=0.705 g=0.696\n",
            ">4, 195/390, d1=0.679, d2=0.699 g=0.693\n",
            ">4, 200/390, d1=0.653, d2=0.705 g=0.696\n",
            ">4, 205/390, d1=0.700, d2=0.678 g=0.726\n",
            ">4, 210/390, d1=0.714, d2=0.697 g=0.712\n",
            ">4, 215/390, d1=0.714, d2=0.682 g=0.708\n",
            ">4, 220/390, d1=0.695, d2=0.701 g=0.711\n",
            ">4, 225/390, d1=0.700, d2=0.678 g=0.714\n",
            ">4, 230/390, d1=0.693, d2=0.691 g=0.705\n",
            ">4, 235/390, d1=0.678, d2=0.711 g=0.701\n",
            ">4, 240/390, d1=0.688, d2=0.679 g=0.709\n",
            ">4, 245/390, d1=0.677, d2=0.730 g=0.691\n",
            ">4, 250/390, d1=0.714, d2=0.679 g=0.716\n",
            ">4, 255/390, d1=0.704, d2=0.678 g=0.706\n",
            ">4, 260/390, d1=0.684, d2=0.704 g=0.699\n",
            ">4, 265/390, d1=0.676, d2=0.693 g=0.710\n",
            ">4, 270/390, d1=0.685, d2=0.686 g=0.708\n",
            ">4, 275/390, d1=0.694, d2=0.689 g=0.714\n",
            ">4, 280/390, d1=0.695, d2=0.691 g=0.716\n",
            ">4, 285/390, d1=0.690, d2=0.689 g=0.706\n",
            ">4, 290/390, d1=0.678, d2=0.711 g=0.697\n",
            ">4, 295/390, d1=0.682, d2=0.686 g=0.710\n",
            ">4, 300/390, d1=0.698, d2=0.679 g=0.720\n",
            ">4, 305/390, d1=0.695, d2=0.690 g=0.705\n",
            ">4, 310/390, d1=0.688, d2=0.663 g=0.740\n",
            ">4, 315/390, d1=0.700, d2=0.701 g=0.703\n",
            ">4, 320/390, d1=0.702, d2=0.697 g=0.719\n",
            ">4, 325/390, d1=0.691, d2=0.685 g=0.708\n",
            ">4, 330/390, d1=0.705, d2=0.690 g=0.714\n",
            ">4, 335/390, d1=0.701, d2=0.661 g=0.724\n",
            ">4, 340/390, d1=0.690, d2=0.690 g=0.712\n",
            ">4, 345/390, d1=0.670, d2=0.725 g=0.706\n",
            ">4, 350/390, d1=0.698, d2=0.702 g=0.709\n",
            ">4, 355/390, d1=0.700, d2=0.681 g=0.708\n",
            ">4, 360/390, d1=0.702, d2=0.682 g=0.710\n",
            ">4, 365/390, d1=0.707, d2=0.672 g=0.720\n",
            ">4, 370/390, d1=0.670, d2=0.736 g=0.719\n",
            ">4, 375/390, d1=0.671, d2=0.733 g=0.688\n",
            ">4, 380/390, d1=0.704, d2=0.675 g=0.730\n",
            ">4, 385/390, d1=0.688, d2=0.707 g=0.698\n",
            ">4, 390/390, d1=0.688, d2=0.709 g=0.700\n",
            ">5, 5/390, d1=0.703, d2=0.675 g=0.734\n",
            ">5, 10/390, d1=0.702, d2=0.676 g=0.716\n",
            ">5, 15/390, d1=0.726, d2=0.672 g=0.730\n",
            ">5, 20/390, d1=0.708, d2=0.676 g=0.722\n",
            ">5, 25/390, d1=0.687, d2=0.676 g=0.718\n",
            ">5, 30/390, d1=0.698, d2=0.663 g=0.725\n",
            ">5, 35/390, d1=0.695, d2=0.698 g=0.716\n",
            ">5, 40/390, d1=0.640, d2=0.684 g=0.714\n",
            ">5, 45/390, d1=0.700, d2=0.674 g=0.744\n",
            ">5, 50/390, d1=0.724, d2=0.690 g=0.711\n",
            ">5, 55/390, d1=0.699, d2=0.690 g=0.705\n",
            ">5, 60/390, d1=0.708, d2=0.668 g=0.721\n",
            ">5, 65/390, d1=0.680, d2=0.718 g=0.710\n",
            ">5, 70/390, d1=0.682, d2=0.741 g=0.711\n",
            ">5, 75/390, d1=0.705, d2=0.688 g=0.703\n",
            ">5, 80/390, d1=0.694, d2=0.688 g=0.709\n",
            ">5, 85/390, d1=0.684, d2=0.697 g=0.704\n",
            ">5, 90/390, d1=0.693, d2=0.706 g=0.711\n",
            ">5, 95/390, d1=0.697, d2=0.674 g=0.724\n",
            ">5, 100/390, d1=0.692, d2=0.702 g=0.697\n",
            ">5, 105/390, d1=0.697, d2=0.704 g=0.701\n",
            ">5, 110/390, d1=0.690, d2=0.693 g=0.704\n",
            ">5, 115/390, d1=0.690, d2=0.677 g=0.718\n",
            ">5, 120/390, d1=0.695, d2=0.691 g=0.705\n",
            ">5, 125/390, d1=0.703, d2=0.681 g=0.714\n",
            ">5, 130/390, d1=0.703, d2=0.681 g=0.709\n",
            ">5, 135/390, d1=0.680, d2=0.685 g=0.699\n",
            ">5, 140/390, d1=0.701, d2=0.703 g=0.703\n",
            ">5, 145/390, d1=0.702, d2=0.670 g=0.724\n",
            ">5, 150/390, d1=0.710, d2=0.687 g=0.710\n",
            ">5, 155/390, d1=0.705, d2=0.677 g=0.710\n",
            ">5, 160/390, d1=0.669, d2=0.705 g=0.685\n",
            ">5, 165/390, d1=0.680, d2=0.691 g=0.717\n",
            ">5, 170/390, d1=0.680, d2=0.689 g=0.717\n",
            ">5, 175/390, d1=0.707, d2=0.684 g=0.715\n",
            ">5, 180/390, d1=0.697, d2=0.678 g=0.722\n",
            ">5, 185/390, d1=0.701, d2=0.682 g=0.708\n",
            ">5, 190/390, d1=0.695, d2=0.679 g=0.707\n",
            ">5, 195/390, d1=0.671, d2=0.699 g=0.694\n",
            ">5, 200/390, d1=0.688, d2=0.682 g=0.721\n",
            ">5, 205/390, d1=0.683, d2=0.700 g=0.740\n",
            ">5, 210/390, d1=0.703, d2=0.719 g=0.709\n",
            ">5, 215/390, d1=0.701, d2=0.679 g=0.719\n",
            ">5, 220/390, d1=0.693, d2=0.680 g=0.712\n",
            ">5, 225/390, d1=0.681, d2=0.709 g=0.694\n",
            ">5, 230/390, d1=0.697, d2=0.686 g=0.714\n",
            ">5, 235/390, d1=0.695, d2=0.694 g=0.709\n",
            ">5, 240/390, d1=0.678, d2=0.712 g=0.711\n",
            ">5, 245/390, d1=0.707, d2=0.665 g=0.724\n",
            ">5, 250/390, d1=0.694, d2=0.712 g=0.703\n",
            ">5, 255/390, d1=0.697, d2=0.692 g=0.708\n",
            ">5, 260/390, d1=0.695, d2=0.703 g=0.713\n",
            ">5, 265/390, d1=0.666, d2=0.685 g=0.708\n",
            ">5, 270/390, d1=0.662, d2=0.700 g=0.687\n",
            ">5, 275/390, d1=0.693, d2=0.679 g=0.733\n",
            ">5, 280/390, d1=0.697, d2=0.692 g=0.700\n",
            ">5, 285/390, d1=0.695, d2=0.684 g=0.709\n",
            ">5, 290/390, d1=0.679, d2=0.700 g=0.702\n",
            ">5, 295/390, d1=0.687, d2=0.690 g=0.701\n",
            ">5, 300/390, d1=0.694, d2=0.694 g=0.714\n",
            ">5, 305/390, d1=0.705, d2=0.674 g=0.733\n",
            ">5, 310/390, d1=0.708, d2=0.711 g=0.721\n",
            ">5, 315/390, d1=0.706, d2=0.692 g=0.705\n",
            ">5, 320/390, d1=0.679, d2=0.689 g=0.704\n",
            ">5, 325/390, d1=0.671, d2=0.713 g=0.691\n",
            ">5, 330/390, d1=0.674, d2=0.701 g=0.706\n",
            ">5, 335/390, d1=0.691, d2=0.696 g=0.699\n",
            ">5, 340/390, d1=0.683, d2=0.701 g=0.699\n",
            ">5, 345/390, d1=0.672, d2=0.692 g=0.712\n",
            ">5, 350/390, d1=0.710, d2=0.682 g=0.719\n",
            ">5, 355/390, d1=0.714, d2=0.670 g=0.721\n",
            ">5, 360/390, d1=0.702, d2=0.689 g=0.719\n",
            ">5, 365/390, d1=0.694, d2=0.682 g=0.706\n",
            ">5, 370/390, d1=0.707, d2=0.681 g=0.725\n",
            ">5, 375/390, d1=0.709, d2=0.695 g=0.710\n",
            ">5, 380/390, d1=0.703, d2=0.693 g=0.711\n",
            ">5, 385/390, d1=0.714, d2=0.684 g=0.717\n",
            ">5, 390/390, d1=0.701, d2=0.696 g=0.715\n",
            ">6, 5/390, d1=0.689, d2=0.682 g=0.719\n",
            ">6, 10/390, d1=0.666, d2=0.698 g=0.715\n",
            ">6, 15/390, d1=0.680, d2=0.696 g=0.706\n",
            ">6, 20/390, d1=0.708, d2=0.667 g=0.728\n",
            ">6, 25/390, d1=0.697, d2=0.689 g=0.717\n",
            ">6, 30/390, d1=0.656, d2=0.714 g=0.705\n",
            ">6, 35/390, d1=0.707, d2=0.680 g=0.718\n",
            ">6, 40/390, d1=0.670, d2=0.762 g=0.679\n",
            ">6, 45/390, d1=0.685, d2=0.697 g=0.720\n",
            ">6, 50/390, d1=0.706, d2=0.672 g=0.732\n",
            ">6, 55/390, d1=0.678, d2=0.689 g=0.702\n",
            ">6, 60/390, d1=0.675, d2=0.730 g=0.693\n",
            ">6, 65/390, d1=0.683, d2=0.690 g=0.707\n",
            ">6, 70/390, d1=0.696, d2=0.721 g=0.705\n",
            ">6, 75/390, d1=0.719, d2=0.674 g=0.721\n",
            ">6, 80/390, d1=0.707, d2=0.690 g=0.712\n",
            ">6, 85/390, d1=0.692, d2=0.694 g=0.708\n",
            ">6, 90/390, d1=0.703, d2=0.686 g=0.707\n",
            ">6, 95/390, d1=0.697, d2=0.687 g=0.713\n",
            ">6, 100/390, d1=0.686, d2=0.687 g=0.716\n",
            ">6, 105/390, d1=0.693, d2=0.690 g=0.710\n",
            ">6, 110/390, d1=0.664, d2=0.712 g=0.696\n",
            ">6, 115/390, d1=0.705, d2=0.697 g=0.712\n",
            ">6, 120/390, d1=0.697, d2=0.690 g=0.716\n",
            ">6, 125/390, d1=0.696, d2=0.683 g=0.716\n",
            ">6, 130/390, d1=0.696, d2=0.686 g=0.713\n",
            ">6, 135/390, d1=0.693, d2=0.680 g=0.726\n",
            ">6, 140/390, d1=0.693, d2=0.690 g=0.725\n",
            ">6, 145/390, d1=0.699, d2=0.684 g=0.721\n",
            ">6, 150/390, d1=0.689, d2=0.684 g=0.716\n",
            ">6, 155/390, d1=0.699, d2=0.711 g=0.702\n",
            ">6, 160/390, d1=0.716, d2=0.681 g=0.719\n",
            ">6, 165/390, d1=0.713, d2=0.677 g=0.715\n",
            ">6, 170/390, d1=0.677, d2=0.690 g=0.710\n",
            ">6, 175/390, d1=0.682, d2=0.692 g=0.738\n",
            ">6, 180/390, d1=0.671, d2=0.798 g=0.701\n",
            ">6, 185/390, d1=0.668, d2=0.688 g=0.739\n",
            ">6, 190/390, d1=0.705, d2=0.676 g=0.753\n",
            ">6, 195/390, d1=0.700, d2=0.688 g=0.719\n",
            ">6, 200/390, d1=0.684, d2=0.697 g=0.701\n",
            ">6, 205/390, d1=0.671, d2=0.719 g=0.704\n",
            ">6, 210/390, d1=0.692, d2=0.699 g=0.716\n",
            ">6, 215/390, d1=0.675, d2=0.690 g=0.700\n",
            ">6, 220/390, d1=0.704, d2=0.707 g=0.736\n",
            ">6, 225/390, d1=0.707, d2=0.692 g=0.709\n",
            ">6, 230/390, d1=0.697, d2=0.697 g=0.716\n",
            ">6, 235/390, d1=0.692, d2=0.685 g=0.706\n",
            ">6, 240/390, d1=0.698, d2=0.682 g=0.717\n",
            ">6, 245/390, d1=0.700, d2=0.668 g=0.721\n",
            ">6, 250/390, d1=0.693, d2=0.689 g=0.710\n",
            ">6, 255/390, d1=0.685, d2=0.692 g=0.719\n",
            ">6, 260/390, d1=0.668, d2=0.685 g=0.727\n",
            ">6, 265/390, d1=0.702, d2=0.666 g=0.716\n",
            ">6, 270/390, d1=0.696, d2=0.691 g=0.707\n",
            ">6, 275/390, d1=0.679, d2=0.720 g=0.699\n",
            ">6, 280/390, d1=0.699, d2=0.721 g=0.688\n",
            ">6, 285/390, d1=0.715, d2=0.655 g=0.763\n",
            ">6, 290/390, d1=0.666, d2=0.707 g=0.688\n",
            ">6, 295/390, d1=0.688, d2=0.710 g=0.710\n",
            ">6, 300/390, d1=0.682, d2=0.702 g=0.717\n",
            ">6, 305/390, d1=0.703, d2=0.692 g=0.711\n",
            ">6, 310/390, d1=0.706, d2=0.696 g=0.710\n",
            ">6, 315/390, d1=0.706, d2=0.679 g=0.706\n",
            ">6, 320/390, d1=0.695, d2=0.692 g=0.709\n",
            ">6, 325/390, d1=0.680, d2=0.692 g=0.705\n",
            ">6, 330/390, d1=0.685, d2=0.684 g=0.730\n",
            ">6, 335/390, d1=0.705, d2=0.668 g=0.748\n",
            ">6, 340/390, d1=0.703, d2=0.675 g=0.717\n",
            ">6, 345/390, d1=0.674, d2=0.681 g=0.712\n",
            ">6, 350/390, d1=0.666, d2=0.722 g=0.704\n",
            ">6, 355/390, d1=0.690, d2=0.671 g=0.735\n",
            ">6, 360/390, d1=0.708, d2=0.676 g=0.722\n",
            ">6, 365/390, d1=0.692, d2=0.682 g=0.711\n",
            ">6, 370/390, d1=0.684, d2=0.708 g=0.715\n",
            ">6, 375/390, d1=0.680, d2=0.693 g=0.725\n",
            ">6, 380/390, d1=0.690, d2=0.680 g=0.732\n",
            ">6, 385/390, d1=0.717, d2=0.683 g=0.718\n",
            ">6, 390/390, d1=0.684, d2=0.675 g=0.713\n",
            ">7, 5/390, d1=0.623, d2=0.719 g=0.680\n",
            ">7, 10/390, d1=0.682, d2=0.695 g=0.719\n",
            ">7, 15/390, d1=0.693, d2=0.706 g=0.707\n",
            ">7, 20/390, d1=0.689, d2=0.701 g=0.718\n",
            ">7, 25/390, d1=0.678, d2=0.700 g=0.706\n",
            ">7, 30/390, d1=0.680, d2=0.704 g=0.712\n",
            ">7, 35/390, d1=0.648, d2=0.681 g=0.709\n",
            ">7, 40/390, d1=0.638, d2=0.822 g=0.660\n",
            ">7, 45/390, d1=0.627, d2=0.768 g=0.701\n",
            ">7, 50/390, d1=0.602, d2=0.709 g=0.711\n",
            ">7, 55/390, d1=0.678, d2=0.709 g=0.702\n",
            ">7, 60/390, d1=0.712, d2=0.663 g=0.766\n",
            ">7, 65/390, d1=0.724, d2=0.687 g=0.728\n",
            ">7, 70/390, d1=0.729, d2=0.692 g=0.726\n",
            ">7, 75/390, d1=0.711, d2=0.666 g=0.727\n",
            ">7, 80/390, d1=0.708, d2=0.686 g=0.723\n",
            ">7, 85/390, d1=0.685, d2=0.692 g=0.704\n",
            ">7, 90/390, d1=0.695, d2=0.694 g=0.725\n",
            ">7, 95/390, d1=0.711, d2=0.665 g=0.734\n",
            ">7, 100/390, d1=0.691, d2=0.679 g=0.718\n",
            ">7, 105/390, d1=0.683, d2=0.717 g=0.696\n",
            ">7, 110/390, d1=0.684, d2=0.695 g=0.720\n",
            ">7, 115/390, d1=0.719, d2=0.680 g=0.744\n",
            ">7, 120/390, d1=0.714, d2=0.705 g=0.714\n",
            ">7, 125/390, d1=0.706, d2=0.674 g=0.721\n",
            ">7, 130/390, d1=0.701, d2=0.689 g=0.705\n",
            ">7, 135/390, d1=0.684, d2=0.690 g=0.718\n",
            ">7, 140/390, d1=0.704, d2=0.668 g=0.728\n",
            ">7, 145/390, d1=0.694, d2=0.689 g=0.720\n",
            ">7, 150/390, d1=0.688, d2=0.725 g=0.714\n",
            ">7, 155/390, d1=0.707, d2=0.664 g=0.730\n",
            ">7, 160/390, d1=0.711, d2=0.668 g=0.720\n",
            ">7, 165/390, d1=0.696, d2=0.677 g=0.721\n",
            ">7, 170/390, d1=0.668, d2=0.676 g=0.721\n",
            ">7, 175/390, d1=0.628, d2=0.735 g=0.706\n",
            ">7, 180/390, d1=0.688, d2=0.731 g=0.705\n",
            ">7, 185/390, d1=0.688, d2=0.702 g=0.709\n",
            ">7, 190/390, d1=0.671, d2=0.682 g=0.752\n",
            ">7, 195/390, d1=0.676, d2=0.701 g=0.693\n",
            ">7, 200/390, d1=0.694, d2=0.693 g=0.722\n",
            ">7, 205/390, d1=0.702, d2=0.687 g=0.703\n",
            ">7, 210/390, d1=0.683, d2=0.702 g=0.705\n",
            ">7, 215/390, d1=0.694, d2=0.690 g=0.724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTMUKlnMFqV9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "4f7f05b9-9f20-40b2-f72f-6a8dc780b79a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-2dd0917708e1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    `# generate points in latent space as input for the generator\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "`# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "\t# generate points in the latent space\n",
        "\tx_input = randn(latent_dim * n_samples)\n",
        "\t# reshape into a batch of inputs for the network\n",
        "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
        "\treturn x_input\n",
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, n):\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tpyplot.subplot(n, n, 1 + i)\n",
        "\t\t# turn off axis\n",
        "\t\tpyplot.axis('off')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tpyplot.imshow(examples[i, :, :])\n",
        "\tpyplot.show()\n",
        "\n",
        "# load model\n",
        "model = gan_model\n",
        "# generate images\n",
        "latent_points = generate_latent_points(100, 100)\n",
        "# generate images\n",
        "X = model.predict(latent_points)\n",
        "# scale from [-1,1] to [0,1]\n",
        "X = (X + 1) / 2.0\n",
        "# plot the result\n",
        "save_plot(X, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRCL4Ju3wjkY"
      },
      "source": [
        "# References\n",
        "\n",
        "Brownlee, Jason. (2019). Generative Adversarial Networks with Python. Ebook. Machine Learning Mastery. \n",
        "\n",
        "Das Shuvo, Falguni. (2020). Repeatedly calling model.predict(...) results in memory leak. GitHub. Retrieved 3/15/2023 from https://github.com/keras-team/keras/issues/13118"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVB9MFpliFRovxQ6UxlVt1",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}